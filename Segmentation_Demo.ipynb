{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9CGh7c7o2+1qNqIw49ccP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/33Surya66/33Surya66/blob/main/Segmentation_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks8GJWXNm7cl",
        "outputId": "b2f3bb5e-cae8-46c8-f72d-8be24769f36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (33.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "     age           job   marital  education default  balance housing loan  \\\n",
            "0     58    management   married   tertiary      no     2143     yes   no   \n",
            "1     44    technician    single  secondary      no       29     yes   no   \n",
            "2     33  entrepreneur   married  secondary      no        2     yes  yes   \n",
            "3     47   blue-collar   married    unknown      no     1506     yes   no   \n",
            "4     33       unknown    single    unknown      no        1      no   no   \n",
            "..   ...           ...       ...        ...     ...      ...     ...  ...   \n",
            "195   33   blue-collar    single  secondary      no      307     yes   no   \n",
            "196   38      services   married  secondary      no      155     yes   no   \n",
            "197   50    technician  divorced   tertiary      no      173      no  yes   \n",
            "198   43    management   married   tertiary      no      400     yes   no   \n",
            "199   61   blue-collar  divorced    primary      no     1428     yes   no   \n",
            "\n",
            "     contact  day  ... duration  campaign  pdays  previous  poutcome   y  \\\n",
            "0    unknown    5  ...      261         1     -1         0   unknown  no   \n",
            "1    unknown    5  ...      151         1     -1         0   unknown  no   \n",
            "2    unknown    5  ...       76         1     -1         0   unknown  no   \n",
            "3    unknown    5  ...       92         1     -1         0   unknown  no   \n",
            "4    unknown    5  ...      198         1     -1         0   unknown  no   \n",
            "..       ...  ...  ...      ...       ...    ...       ...       ...  ..   \n",
            "195  unknown    5  ...      309         2     -1         0   unknown  no   \n",
            "196  unknown    5  ...      248         1     -1         0   unknown  no   \n",
            "197  unknown    5  ...       98         1     -1         0   unknown  no   \n",
            "198  unknown    5  ...      256         1     -1         0   unknown  no   \n",
            "199  unknown    5  ...       82         2     -1         0   unknown  no   \n",
            "\n",
            "          Phone                    Email  \\\n",
            "0    8484767988   taylorpage@example.com   \n",
            "1    6165567443    meganhall@example.org   \n",
            "2    8217310147      derek30@example.org   \n",
            "3    9234549757     gerald83@example.org   \n",
            "4    4483924542     amendoza@example.com   \n",
            "..          ...                      ...   \n",
            "195  8317626216       oallen@example.org   \n",
            "196  5299215189    bodonnell@example.net   \n",
            "197  3425685864     uoconnor@example.com   \n",
            "198  9438641901      rcortez@example.org   \n",
            "199  3362671352  sparkswanda@example.com   \n",
            "\n",
            "                                               Address Has_4_Wheeler  \n",
            "0                              USNV Cruz\\nFPO AA 14966         False  \n",
            "1                3371 Jean Loop\\nLake Joshua, MI 19338         False  \n",
            "2    98276 William Orchard Apt. 541\\nNew Rachelvill...          True  \n",
            "3                             USS Prince\\nFPO AE 48388         False  \n",
            "4    4754 James Expressway Suite 021\\nBushborough, ...         False  \n",
            "..                                                 ...           ...  \n",
            "195  449 Kelly Field Apt. 297\\nLake Rachelmouth, NH...         False  \n",
            "196         0982 Martinez Center\\nLake Jerry, NE 01197         False  \n",
            "197  52946 Massey Mountains Suite 709\\nRobertsonmou...          True  \n",
            "198            314 Davis Centers\\nDeannabury, NY 02971          True  \n",
            "199              591 Pierce Unions\\nGrayfort, TX 77099          True  \n",
            "\n",
            "[200 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "!pip install faker  # Install faker if you haven't already\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "\n",
        "def generate_10_digit_phone():\n",
        "    \"\"\"Generates a random 10-digit phone number.\"\"\"\n",
        "    fake = Faker()\n",
        "    while True:\n",
        "        phone = fake.phone_number()\n",
        "        # Remove non-digit characters and check length\n",
        "        cleaned_phone = re.sub(r'\\D', '', phone)\n",
        "        if len(cleaned_phone) == 10:\n",
        "            return cleaned_phone\n",
        "\n",
        "def has_4_wheeler(age):\n",
        "    \"\"\"Randomly determines if a person has a 4-wheeler based on age.\"\"\"\n",
        "    fake = Faker()\n",
        "    if age > 18:\n",
        "        return fake.boolean(chance_of_getting_true=50)  # 50% chance\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Load your CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv('/content/train.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file '/content/train.csv' was not found.\")\n",
        "    # Handle the file not found error, e.g., exit or use a sample DataFrame\n",
        "    df = pd.DataFrame() # Create an empty DataFrame if the file is not found\n",
        "\n",
        "# Add new columns with fake data using Faker methods\n",
        "if not df.empty: # Proceed only if the DataFrame is not empty\n",
        "    df['Phone'] = [generate_10_digit_phone() for _ in range(len(df))]  # Use the custom function\n",
        "    df['Email'] = [fake.email() for _ in range(len(df))]\n",
        "    df['Address'] = [fake.address() for _ in range(len(df))]\n",
        "    # ... add more columns with other Faker providers as needed ...\n",
        "\n",
        "    # Add 'Has_4_Wheeler' column based on age\n",
        "    df['Has_4_Wheeler'] = df['age'].apply(has_4_wheeler)  # Assuming you have an 'Age' column\n",
        "\n",
        "    print(df.head(200)) # Print the first few rows of the updated DataFrame\n",
        "# ... (rest of your code)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(200))\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTwH6nA7oVb4",
        "outputId": "35e0dded-99f2-433b-c1ba-60d140ff517d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      age           job   marital  education default  balance housing loan  \\\n",
            "0    58.0    management   married   tertiary      no   2143.0     yes   no   \n",
            "1    44.0    technician    single  secondary      no     29.0     yes   no   \n",
            "2    33.0  entrepreneur   married  secondary      no      2.0     yes  yes   \n",
            "3    47.0   blue-collar   married    unknown      no   1506.0     yes   no   \n",
            "4    33.0       unknown    single    unknown      no      1.0      no   no   \n",
            "..    ...           ...       ...        ...     ...      ...     ...  ...   \n",
            "195  33.0   blue-collar    single  secondary      no    307.0     yes   no   \n",
            "196  38.0      services   married  secondary      no    155.0     yes   no   \n",
            "197  50.0    technician  divorced   tertiary      no    173.0      no  yes   \n",
            "198  43.0    management   married   tertiary      no    400.0     yes   no   \n",
            "199  61.0   blue-collar  divorced    primary      no   1428.0     yes   no   \n",
            "\n",
            "     contact  day month  duration  campaign  pdays  previous poutcome   y  \\\n",
            "0    unknown    5   may     261.0       1.0   -1.0       0.0  unknown  no   \n",
            "1    unknown    5   may     151.0       1.0   -1.0       0.0  unknown  no   \n",
            "2    unknown    5   may      76.0       1.0   -1.0       0.0  unknown  no   \n",
            "3    unknown    5   may      92.0       1.0   -1.0       0.0  unknown  no   \n",
            "4    unknown    5   may     198.0       1.0   -1.0       0.0  unknown  no   \n",
            "..       ...  ...   ...       ...       ...    ...       ...      ...  ..   \n",
            "195  unknown    5   may     309.0       2.0   -1.0       0.0  unknown  no   \n",
            "196  unknown    5   may     248.0       1.0   -1.0       0.0  unknown  no   \n",
            "197  unknown    5   may      98.0       1.0   -1.0       0.0  unknown  no   \n",
            "198  unknown    5   may     256.0       1.0   -1.0       0.0  unknown  no   \n",
            "199  unknown    5   may      82.0       2.0   -1.0       0.0  unknown  no   \n",
            "\n",
            "          Phone                         Email  \\\n",
            "0    7348360819            eweeks@example.net   \n",
            "1    8147281184      stephensmith@example.com   \n",
            "2    8298560443           kmullen@example.org   \n",
            "3    4324042362      gailgarrison@example.org   \n",
            "4    5346389899    timothymarquez@example.org   \n",
            "..          ...                           ...   \n",
            "195  8396775108       rhondameyer@example.com   \n",
            "196  7175869976  maldonadodesiree@example.net   \n",
            "197  3575650817         jessica73@example.net   \n",
            "198  7349624528            sbauer@example.net   \n",
            "199  3923792144    elainegonzalez@example.net   \n",
            "\n",
            "                                               Address  \n",
            "0     02093 Mary Crossroad\\nNorth Autumnview, MD 01896  \n",
            "1            4695 Chelsea Viaduct\\nMunozbury, WY 76924  \n",
            "2         23616 Gilmore Club\\nRichardsonside, SC 81775  \n",
            "3       58756 Harvey Burg Apt. 546\\nNew Ryan, MD 62079  \n",
            "4     86305 Theresa Port Apt. 470\\nSarahberg, NV 92045  \n",
            "..                                                 ...  \n",
            "195  6478 Anderson Pike Apt. 849\\nNorth Arthur, MA ...  \n",
            "196  105 Sandra Hills Apt. 624\\nGonzalezside, ID 71931  \n",
            "197       124 Wayne Brooks\\nWest Gregoryport, IN 42818  \n",
            "198          6971 Thompson Fields\\nAlexiston, OH 47235  \n",
            "199     98190 Sandoval Lakes\\nEast Katherine, SD 03825  \n",
            "\n",
            "[200 rows x 20 columns]\n",
            "45211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: clean data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'df' is your DataFrame (loaded previously)\n",
        "# Replace NaN values with a specific value (e.g., 0)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Remove duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "# Convert columns to appropriate data types\n",
        "for col in df.columns:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        # Handle non-numeric values in numeric columns\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col] = df[col].fillna(0)  # Fill NaN values with 0\n",
        "    elif pd.api.types.is_string_dtype(df[col]):\n",
        "        df[col] = df[col].str.strip() # Remove leading/trailing spaces\n",
        "        # Handle invalid strings (e.g., replace with empty string)\n",
        "        df[col] = df[col].replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
        "\n",
        "# Example of outlier removal (using IQR method)\n",
        "def remove_outliers_iqr(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data >= lower_bound) & (data <= upper_bound)]\n",
        "\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col] = remove_outliers_iqr(df[col])\n",
        "\n",
        "# Fill NaN values resulting from outlier removal with mean for numeric columns\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "print(df.head(200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sgfuoagrcAx",
        "outputId": "c1d1d8a3-98bc-49fe-fb5d-a16aaddbc83d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      age           job   marital  education default  balance housing loan  \\\n",
            "0    58.0    management   married   tertiary      no   2143.0     yes   no   \n",
            "1    44.0    technician    single  secondary      no     29.0     yes   no   \n",
            "2    33.0  entrepreneur   married  secondary      no      2.0     yes  yes   \n",
            "3    47.0   blue-collar   married    unknown      no   1506.0     yes   no   \n",
            "4    33.0       unknown    single    unknown      no      1.0      no   no   \n",
            "..    ...           ...       ...        ...     ...      ...     ...  ...   \n",
            "195  33.0   blue-collar    single  secondary      no    307.0     yes   no   \n",
            "196  38.0      services   married  secondary      no    155.0     yes   no   \n",
            "197  50.0    technician  divorced   tertiary      no    173.0      no  yes   \n",
            "198  43.0    management   married   tertiary      no    400.0     yes   no   \n",
            "199  61.0   blue-collar  divorced    primary      no   1428.0     yes   no   \n",
            "\n",
            "     contact  day month  duration  campaign  pdays  previous poutcome   y  \\\n",
            "0    unknown    5   may     261.0       1.0   -1.0       0.0  unknown  no   \n",
            "1    unknown    5   may     151.0       1.0   -1.0       0.0  unknown  no   \n",
            "2    unknown    5   may      76.0       1.0   -1.0       0.0  unknown  no   \n",
            "3    unknown    5   may      92.0       1.0   -1.0       0.0  unknown  no   \n",
            "4    unknown    5   may     198.0       1.0   -1.0       0.0  unknown  no   \n",
            "..       ...  ...   ...       ...       ...    ...       ...      ...  ..   \n",
            "195  unknown    5   may     309.0       2.0   -1.0       0.0  unknown  no   \n",
            "196  unknown    5   may     248.0       1.0   -1.0       0.0  unknown  no   \n",
            "197  unknown    5   may      98.0       1.0   -1.0       0.0  unknown  no   \n",
            "198  unknown    5   may     256.0       1.0   -1.0       0.0  unknown  no   \n",
            "199  unknown    5   may      82.0       2.0   -1.0       0.0  unknown  no   \n",
            "\n",
            "          Phone                         Email  \\\n",
            "0    7348360819            eweeks@example.net   \n",
            "1    8147281184      stephensmith@example.com   \n",
            "2    8298560443           kmullen@example.org   \n",
            "3    4324042362      gailgarrison@example.org   \n",
            "4    5346389899    timothymarquez@example.org   \n",
            "..          ...                           ...   \n",
            "195  8396775108       rhondameyer@example.com   \n",
            "196  7175869976  maldonadodesiree@example.net   \n",
            "197  3575650817         jessica73@example.net   \n",
            "198  7349624528            sbauer@example.net   \n",
            "199  3923792144    elainegonzalez@example.net   \n",
            "\n",
            "                                               Address  \n",
            "0     02093 Mary Crossroad\\nNorth Autumnview, MD 01896  \n",
            "1            4695 Chelsea Viaduct\\nMunozbury, WY 76924  \n",
            "2         23616 Gilmore Club\\nRichardsonside, SC 81775  \n",
            "3       58756 Harvey Burg Apt. 546\\nNew Ryan, MD 62079  \n",
            "4     86305 Theresa Port Apt. 470\\nSarahberg, NV 92045  \n",
            "..                                                 ...  \n",
            "195  6478 Anderson Pike Apt. 849\\nNorth Arthur, MA ...  \n",
            "196  105 Sandra Hills Apt. 624\\nGonzalezside, ID 71931  \n",
            "197       124 Wayne Brooks\\nWest Gregoryport, IN 42818  \n",
            "198          6971 Thompson Fields\\nAlexiston, OH 47235  \n",
            "199     98190 Sandoval Lakes\\nEast Katherine, SD 03825  \n",
            "\n",
            "[200 rows x 20 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: resulting number of rows\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ApNZ1r1r94L",
        "outputId": "fbd9108f-c746-4dfa-dcfa-8cbe96340d09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: clean data further\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "!pip install faker  # Install faker if you haven't already\n",
        "\n",
        "def generate_10_digit_phone():\n",
        "    \"\"\"Generates a random 10-digit phone number.\"\"\"\n",
        "    fake = Faker()\n",
        "    while True:\n",
        "        phone = fake.phone_number()\n",
        "        cleaned_phone = re.sub(r'\\D', '', phone)\n",
        "        if len(cleaned_phone) == 10:\n",
        "            return cleaned_phone\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/train.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file '/content/train.csv' was not found.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "fake = Faker() # Initialize Faker object outside the loop for better performance\n",
        "\n",
        "if not df.empty:\n",
        "    df['Phone'] = [generate_10_digit_phone() for _ in range(len(df))]\n",
        "    df['Email'] = [fake.email() for _ in range(len(df))]\n",
        "    df['Address'] = [fake.address() for _ in range(len(df))]\n",
        "\n",
        "    # ... other column additions ...\n",
        "\n",
        "# Data Cleaning Enhancements\n",
        "if not df.empty:\n",
        "    # Handle missing values more robustly\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            # Use median instead of mean for outlier-resistant imputation\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "        elif pd.api.types.is_string_dtype(df[col]):\n",
        "            df[col] = df[col].astype(str).str.strip()\n",
        "            df[col] = df[col].replace(r'[^\\x00-\\x7F]+', '', regex=True) # Remove non-ASCII chars\n",
        "            df[col] = df[col].fillna('') # Fill NaN with empty string\n",
        "\n",
        "    # Improved outlier removal (handling potential errors gracefully)\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        try:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            df[col] = df[col][(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "            df[col] = df[col].fillna(df[col].median()) # Fill with median after outlier removal\n",
        "        except Exception as e: #Catch any exception during outlier removal\n",
        "            print(f\"Error processing column '{col}': {e}\")\n",
        "\n",
        "    # Remove duplicate rows (after cleaning)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "print(df.head(200))\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "wfce7BdLtizu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install PyPDF2  # Install PyPDF2 for PDF extraction\n",
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "# Function to clean text (remove special characters, numbers, and stopwords)\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to load user data from PDF (assuming tabular data)\n",
        "def load_user_data_from_pdf(pdf_file):\n",
        "    # Assume the PDF contains tabular data that can be converted to a DataFrame\n",
        "    pdf_text = extract_text_from_pdf(pdf_file)\n",
        "    # ... (Logic to convert pdf_text to a DataFrame)\n",
        "    # You may need to use libraries like camelot, tabula-py, or regular expressions\n",
        "    # to extract the tabular data from the PDF text.\n",
        "    # Example (using pandas read_csv with a StringIO object):\n",
        "    # from io import StringIO\n",
        "    # df = pd.read_csv(StringIO(pdf_text), separator=',')\n",
        "    # ... (Other processing to clean and format the DataFrame)\n",
        "    return df  # Return the created DataFrame\n",
        "\n",
        "# Function to match users to the policy based on text similarity\n",
        "def find_target_audience(user_data, policy_text, user_text_column='Description'):  # Assuming 'Description' column in user data\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    user_vectors = vectorizer.fit_transform(user_data[user_text_column].fillna(''))\n",
        "    policy_vector = vectorizer.transform([policy_text])\n",
        "    similarities = cosine_similarity(user_vectors, policy_vector)\n",
        "    user_data['Relevance'] = similarities\n",
        "    sorted_users = user_data.sort_values(by='Relevance', ascending=False)\n",
        "    return sorted_users\n",
        "\n",
        "# Function to save results to a CSV file\n",
        "def save_results(target_audience, output_file):\n",
        "    target_audience.to_csv(output_file, index=False)\n",
        "    print(f\"Target audience saved to {output_file}\")\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    user_data_pdf = '/content/train.csv'  # Path to the user data PDF\n",
        "    policy_pdf = '/content/policy_document.pdf/content/bajaj-allianz-car-insurance-policy-brochurepdf.pdf'  # Path to the policy document PDF\n",
        "    output_file = '/content/target_audience.csv'  # Path to save the results\n",
        "\n",
        "    try:\n",
        "        # Load user data from PDF\n",
        "        user_data = load_user_data_from_pdf(user_data_pdf)\n",
        "        print(\"User data loaded successfully!\")\n",
        "\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)  # Clean the policy text\n",
        "        print(\"Policy text extracted and cleaned successfully!\")\n",
        "\n",
        "        # Find target audience\n",
        "        target_audience = find_target_audience(user_data, policy_text)\n",
        "        print(\"Target audience identified successfully!\")\n",
        "\n",
        "        # Save results\n",
        "        save_results(target_audience, output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mTL_9P0tvyd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install faker  # Install faker if you haven't already\n",
        "!pip install PyPDF2  # Install PyPDF2 for PDF extraction\n",
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def generate_10_digit_phone():\n",
        "    \"\"\"Generates a random 10-digit phone number.\"\"\"\n",
        "    fake = Faker()\n",
        "    while True:\n",
        "        phone = fake.phone_number()\n",
        "        # Remove non-digit characters and check length\n",
        "        cleaned_phone = re.sub(r'\\D', '', phone)\n",
        "        if len(cleaned_phone) == 10:\n",
        "            return cleaned_phone\n",
        "\n",
        "\n",
        "def has_4_wheeler(age):\n",
        "    \"\"\"Randomly determines if a person has a 4-wheeler based on age.\"\"\"\n",
        "    fake = Faker()\n",
        "    if age > 18:\n",
        "        return fake.boolean(chance_of_getting_true=50)  # 50% chance\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def load_user_data_from_csv(csv_file):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        # Add new columns with fake data using Faker methods\n",
        "        if not df.empty:\n",
        "            df['Phone'] = [generate_10_digit_phone()\n",
        "                          for _ in range(len(df))]\n",
        "            df['Email'] = [fake.email() for _ in range(len(df))]\n",
        "            df['Address'] = [fake.address() for _ in range(len(df))]\n",
        "            # Add 'Has_4_Wheeler' column based on age\n",
        "            df['Has_4_Wheeler'] = df['age'].apply(has_4_wheeler)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{csv_file}' was not found.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if file not found\n",
        "\n",
        "\n",
        "def find_target_audience(user_data, policy_text, user_text_column='Description'):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    user_vectors = vectorizer.fit_transform(\n",
        "        user_data[user_text_column].fillna(''))\n",
        "    policy_vector = vectorizer.transform([policy_text])\n",
        "    similarities = cosine_similarity(user_vectors, policy_vector)\n",
        "    user_data['Relevance'] = similarities\n",
        "    sorted_users = user_data.sort_values(by='Relevance', ascending=False)\n",
        "    return sorted_users\n",
        "\n",
        "\n",
        "def save_results(target_audience, output_file):\n",
        "    target_audience.to_csv(output_file, index=False)\n",
        "    print(f\"Target audience saved to {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_data_csv = '/content/train.csv'\n",
        "    policy_pdf = '/content/bajaj-allianz-car-insurance-policy-brochurepdf.pdf'\n",
        "    output_file = '/content/target_audience.csv'\n",
        "\n",
        "    try:\n",
        "        # Load user data from CSV\n",
        "        user_data = load_user_data_from_csv(user_data_csv)\n",
        "        print(\"User data loaded successfully!\")\n",
        "\n",
        "        # Filter users who do not have a 4-wheeler\n",
        "        user_data_no_4wheeler = user_data[user_data['Has_4_Wheeler'] == False]\n",
        "        print(\"Filtered users without 4-wheelers.\")\n",
        "\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)\n",
        "        print(\"Policy text extracted and cleaned successfully!\")\n",
        "\n",
        "        # Find target audience (from filtered users)\n",
        "        target_audience = find_target_audience(\n",
        "            user_data_no_4wheeler, policy_text)\n",
        "        print(\"Target audience identified successfully!\")\n",
        "\n",
        "\n",
        "        # Save results\n",
        "        save_results(target_audience, output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fake = Faker()  # Initialize Faker object outside the loop\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1u1wb8LwQcl",
        "outputId": "07ef1495-8250-44ac-b672-7d8e39e0d175"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (33.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User data loaded successfully!\n",
            "Filtered users without 4-wheelers.\n",
            "Policy text extracted and cleaned successfully!\n",
            "Error: 'Description'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def main():\n",
        "    policy_pdf = '/content/bajaj-allianz-car-insurance-policy-brochurepdf.pdf'\n",
        "\n",
        "    try:\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)  # Clean the policy text\n",
        "        print(policy_text) # Print the extracted and cleaned policy text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHIewBgA0g88",
        "outputId": "cd3f219d-78a3-4c5c-ccee-095d07603385"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bajaj allianz general insurance co ltd bajaj allianz house airport road yerawada pune irda reg query toll free www bajajallianz com bagichelp bajajallianz co policy holders download insurance w allet one touch access vailable details risk factors terms conditions please read sales brochure concluding sale cin u pn plc bjaz b feb cin u pn plcbajaj allianz private car package policy ensuring iles smiles private car package policy owning car become affordable days service maintenance become expensive costs owner long run especially car damaged due unavoidable circumstances accident private car package policy designed keeping mind situations car protected need coverages section damage cover accidental loss damage car caused following fire explosion self ignition lightning burglary housebreaking theft riot strike earthquake fire shock damage flood typhoon hurricane storm tempest inundation cyclone hailstorm frost accidental external means malicious act terrorist activity whilst transit road rail inland waterway lift elevator air landslide rockslide section liability third parties cover legal liability arising bodily injury property damage third parties caused due accident involving car section personal accident cover unfortunate event owner driver death permanent total disability arising accident traveling car pay specified sum insured legal heir optional extensions also opt following extensions part private car package policy loss accessories legal liability paid driver cleaner workman personal accident cover occupants main exclusions liable pay following cases loss damage caused outside geographical area loss damage arising car used per limitations use claim arising contractual liability consequential loss exclusions listed policy wordings discounts discounts available private car package policy claims free experience opting voluntary excess membership approved automobile associations installing approved anti theft devices premium get estimate premium payable car kindly fill private car package policy proposal form based information furnished shall inform premium amount paid claims process call toll free submit completely filled claim form nearest bajaj allianz general insurance office bajaj allianz bajaj allianz joint venture bajaj finserv limited allianz se enjoy reputation expertise stability strength joint venture company incorporates global expertise local experience comprehensive innovative solutions combine technical expertise experience allianz se indepth market knowledge goodwill bajaj brand india competitive pricing quick honest response earned company customer trust market leadership short time disclaimer mentioned information indicative nature details coverage exclusions please refer policy wordings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: column names\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf0etxz417u_",
        "outputId": "39eec3dc-532c-455e-b2a4-379d02974c9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
              "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
              "       'previous', 'poutcome', 'y', 'Phone', 'Email', 'Address',\n",
              "       'Has_4_Wheeler'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_columns_for_text(data, columns):\n",
        "    \"\"\"Combine multiple columns into a single text feature for relevance analysis.\"\"\"\n",
        "    return data[columns].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "\n",
        "def find_target_audience(user_data, policy_text, combined_column_name='CombinedDescription'):\n",
        "    # Ensure combined_column_name exists\n",
        "    if combined_column_name not in user_data.columns:\n",
        "        raise ValueError(f\"Column '{combined_column_name}' not found in user_data.\")\n",
        "\n",
        "    # Fill NaN values with an empty string to avoid issues\n",
        "    user_data[combined_column_name] = user_data[combined_column_name].fillna('')\n",
        "\n",
        "    # Vectorize user data and policy text\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    user_vectors = vectorizer.fit_transform(user_data[combined_column_name])\n",
        "    policy_vector = vectorizer.transform([policy_text])\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = cosine_similarity(user_vectors, policy_vector).flatten()\n",
        "    user_data['Relevance'] = similarities\n",
        "\n",
        "    # Sort users by relevance\n",
        "    sorted_users = user_data.sort_values(by='Relevance', ascending=False)\n",
        "    return sorted_users\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_data_csv = '/content/train.csv'\n",
        "    policy_pdf = '/content/bajaj-allianz-car-insurance-policy-brochurepdf.pdf'\n",
        "    output_file = '/content/target_audience.csv'\n",
        "\n",
        "    try:\n",
        "        # Load user data from CSV\n",
        "        user_data = load_user_data_from_csv(user_data_csv)\n",
        "        print(\"User data loaded successfully!\")\n",
        "\n",
        "        # Filter users who do not have a 4-wheeler\n",
        "        user_data_no_4wheeler = user_data[user_data['Has_4_Wheeler'] == False]\n",
        "        print(\"Filtered users without 4-wheelers.\")\n",
        "\n",
        "        # Combine selected columns into a single column for relevance analysis\n",
        "        relevant_columns = [\n",
        "            'age', 'job', 'marital', 'education', 'default', 'balance',\n",
        "            'housing', 'loan', 'contact', 'day', 'month', 'duration',\n",
        "            'campaign', 'pdays', 'previous', 'poutcome', 'y', 'Phone',\n",
        "            'Email', 'Address', 'Has_4_Wheeler'\n",
        "        ]\n",
        "        user_data_no_4wheeler['CombinedDescription'] = combine_columns_for_text(user_data_no_4wheeler, relevant_columns)\n",
        "        print(\"Combined relevant columns into 'CombinedDescription'.\")\n",
        "\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)\n",
        "        print(\"Policy text extracted and cleaned successfully!\")\n",
        "\n",
        "        # Find target audience using combined columns\n",
        "        target_audience = find_target_audience(user_data_no_4wheeler, policy_text)\n",
        "        print(\"Target audience identified successfully!\")\n",
        "\n",
        "        # Save results\n",
        "        save_results(target_audience, output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "p4SVdH6Y2nNN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install faker  # Install faker if you haven't already\n",
        "!pip install PyPDF2  # Install PyPDF2 for PDF extraction\n",
        "!pip install nltk\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def generate_10_digit_phone():\n",
        "    \"\"\"Generates a random 10-digit phone number.\"\"\"\n",
        "    fake = Faker()\n",
        "    while True:\n",
        "        phone = fake.phone_number()\n",
        "        # Remove non-digit characters and check length\n",
        "        cleaned_phone = re.sub(r'\\D', '', phone)\n",
        "        if len(cleaned_phone) == 10:\n",
        "            return cleaned_phone\n",
        "\n",
        "\n",
        "def has_4_wheeler(age):\n",
        "    \"\"\"Randomly determines if a person has a 4-wheeler based on age.\"\"\"\n",
        "    fake = Faker()\n",
        "    if age > 18:\n",
        "        return fake.boolean(chance_of_getting_true=50)  # 50% chance\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing non-alphanumeric characters and stop words.\"\"\"\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def load_user_data_from_csv(csv_file):\n",
        "    \"\"\"Loads user data from a CSV file and adds generated data.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        if not df.empty:\n",
        "            fake = Faker()\n",
        "            # Add new columns with fake data\n",
        "            df['Phone'] = [generate_10_digit_phone() for _ in range(len(df))]\n",
        "            df['Email'] = [fake.email() for _ in range(len(df))]\n",
        "            df['Address'] = [fake.address() for _ in range(len(df))]\n",
        "            # Add 'Has_4_Wheeler' column based on age\n",
        "            df['Has_4_Wheeler'] = df['age'].apply(has_4_wheeler)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{csv_file}' was not found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def find_target_audience(user_data, policy_text, user_text_column='Has_4_Wheeler'):\n",
        "    \"\"\"Finds the target audience based on cosine similarity.\"\"\"\n",
        "    # Convert the 'Has_4_Wheeler' column to string\n",
        "    user_data[user_text_column] = user_data[user_text_column].astype(str)\n",
        "\n",
        "    # Vectorize user data and policy text\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    user_vectors = vectorizer.fit_transform(user_data[user_text_column])\n",
        "    policy_vector = vectorizer.transform([policy_text])\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = cosine_similarity(user_vectors, policy_vector).flatten()\n",
        "    user_data['Relevance'] = similarities\n",
        "\n",
        "    # Sort users by relevance\n",
        "    sorted_users = user_data.sort_values(by='Relevance', ascending=False)\n",
        "    return sorted_users\n",
        "\n",
        "\n",
        "def save_results(target_audience, output_file):\n",
        "    \"\"\"Saves the target audience to a CSV file.\"\"\"\n",
        "    target_audience.to_csv(output_file, index=False)\n",
        "    print(f\"Target audience saved to {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_data_csv = '/content/train.csv'\n",
        "    policy_pdf = '/content/bajaj-allianz-car-insurance-policy-brochurepdf.pdf'\n",
        "    output_file = '/content/target_audience.csv'\n",
        "\n",
        "    try:\n",
        "        # Load user data from CSV\n",
        "        user_data = load_user_data_from_csv(user_data_csv)\n",
        "        print(\"User data loaded successfully!\")\n",
        "\n",
        "        # Filter users who do not have a 4-wheeler\n",
        "        user_data_no_4wheeler = user_data[user_data['Has_4_Wheeler'] == False]\n",
        "        print(\"Filtered users without 4-wheelers.\")\n",
        "\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)\n",
        "        print(\"Policy text extracted and cleaned successfully!\")\n",
        "\n",
        "        # Find target audience using 'Has_4_Wheeler' column\n",
        "        target_audience = find_target_audience(user_data_no_4wheeler, policy_text, user_text_column='Has_4_Wheeler')\n",
        "        print(\"Target audience identified successfully!\")\n",
        "\n",
        "        # Save results\n",
        "        save_results(target_audience, output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hxV4wMG3MeU",
        "outputId": "427ff5df-ed75-404d-85b2-372109bd37a2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (33.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User data loaded successfully!\n",
            "Filtered users without 4-wheelers.\n",
            "Policy text extracted and cleaned successfully!\n",
            "Target audience identified successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-5d7a56da5cf7>:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  user_data[user_text_column] = user_data[user_text_column].astype(str)\n",
            "<ipython-input-27-5d7a56da5cf7>:94: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  user_data['Relevance'] = similarities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target audience saved to /content/target_audience.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install faker\n",
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import re\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def generate_10_digit_phone():\n",
        "    \"\"\"Generates a random 10-digit phone number.\"\"\"\n",
        "    fake = Faker()\n",
        "    while True:\n",
        "        phone = fake.phone_number()\n",
        "        cleaned_phone = re.sub(r'\\D', '', phone)\n",
        "        if len(cleaned_phone) == 10:\n",
        "            return cleaned_phone\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text by removing non-alphanumeric characters and stop words.\"\"\"\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "def load_user_data_from_csv(csv_file):\n",
        "    \"\"\"Loads user data from a CSV file and adds generated data.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        if not df.empty:\n",
        "            fake = Faker()\n",
        "            # Add new columns with fake data\n",
        "            df['Phone'] = [generate_10_digit_phone() for _ in range(len(df))]\n",
        "            df['Email'] = [fake.email() for _ in range(len(df))]\n",
        "            df['Address'] = [fake.address() for _ in range(len(df))]\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{csv_file}' was not found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def find_target_audience(user_data, policy_text, user_text_column='housing'):\n",
        "    \"\"\"Finds the target audience based on cosine similarity.\"\"\"\n",
        "    # Convert the 'housing' column to string\n",
        "    user_data[user_text_column] = user_data[user_text_column].astype(str)\n",
        "\n",
        "    # Vectorize user data and policy text\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    user_vectors = vectorizer.fit_transform(user_data[user_text_column])\n",
        "    policy_vector = vectorizer.transform([policy_text])\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = cosine_similarity(user_vectors, policy_vector).flatten()\n",
        "    user_data['Relevance'] = similarities\n",
        "\n",
        "    # Sort users by relevance\n",
        "    sorted_users = user_data.sort_values(by='Relevance', ascending=False)\n",
        "    return sorted_users\n",
        "\n",
        "\n",
        "def save_results(target_audience, output_file):\n",
        "    \"\"\"Saves the target audience to a CSV file.\"\"\"\n",
        "    target_audience.to_csv(output_file, index=False)\n",
        "    print(f\"Target audience saved to {output_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    user_data_csv = '/content/train.csv'\n",
        "    policy_pdf = '/content/home-loan-policy.pdf'\n",
        "    output_file = '/content/target_audience_home_loan.csv'\n",
        "\n",
        "    try:\n",
        "        # Load user data from CSV\n",
        "        user_data = load_user_data_from_csv(user_data_csv)\n",
        "        print(\"User data loaded successfully!\")\n",
        "\n",
        "        # Filter users who do not have a housing loan\n",
        "        user_data_no_housing_loan = user_data[user_data['housing'] == 'no']\n",
        "        print(\"Filtered users without a housing loan.\")\n",
        "\n",
        "        # Extract policy text from PDF\n",
        "        policy_text = extract_text_from_pdf(policy_pdf)\n",
        "        policy_text = clean_text(policy_text)\n",
        "        print(\"Policy text extracted and cleaned successfully!\")\n",
        "\n",
        "        # Find target audience using 'housing' column\n",
        "        target_audience = find_target_audience(user_data_no_housing_loan, policy_text, user_text_column='housing')\n",
        "        print(\"Target audience identified successfully!\")\n",
        "\n",
        "        # Save results\n",
        "        save_results(target_audience, output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH0LJH8E5cpC",
        "outputId": "609a1a1b-9571-4a5c-e617-45d77a40cc2c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (33.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.11/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User data loaded successfully!\n",
            "Filtered users without a housing loan.\n",
            "Error: [Errno 2] No such file or directory: '/content/home-loan-policy.pdf'\n"
          ]
        }
      ]
    }
  ]
}